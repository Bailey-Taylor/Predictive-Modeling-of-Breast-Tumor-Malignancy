---
output: html_document
---
# Predictive Modeling of Breast Tumor Malignancy

## Bailey Taylor

### Introduction

According to the American Cancer Society, breast cancer stands as the second leading cause of cancer-related deaths in women. Given its severity, early detection is crucial for effective treatment and improved patient outcomes. This study aims to evaluate the accuracy of predicting breast tumor malignancy using morphological and textural features of tumor cells, utilizing data from the University of Wisconsin Diagnostic Breast Cancer dataset.

### Part 1: Data Preparation

```{r}
# import data 
breast_cancer <- read.csv("breast-cancer.csv")
```

```{r}
# first 6 rows 
head(breast_cancer)
```

```{r}
# remove ID column
breast_cancer1 <- breast_cancer[, -c(1,13:32)]
head(breast_cancer1)

# remove _se and _worst columns 
```

```{r}
# check for missing values 
sum(is.na(breast_cancer1))
```

```{r}
# Dummy code target variable "diagnosis", M = 1, B = 0
breast_cancer1$diagnosis <- ifelse(breast_cancer$diagnosis == "M", 1, 0)
head(breast_cancer1, 20)
```

### Part 2: Visualize the Data

```{r}
# descriptive statistics 
summary(breast_cancer1)

```

```{r}
# Count of M and B
count<- table(breast_cancer1$diagnosis)
count

percentages <- round(100 * count / sum(count), 1)
percentages_label <- paste(c("B", "M"), "\n", percentages, "%", sep = "")
pie(count, labels = percentages_label, col = c('#00AFBB', '#E7B800'),
    main = "Distribution of Diagnosis")
```

```{r}

# Histograms
library(Hmisc)
hist.data.frame(breast_cancer1)



# Histograms 
library(ggplot2)

h1 <- ggplot(breast_cancer1, aes(x = radius_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Radius Mean",
       x = "Radius Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h2 <- ggplot(breast_cancer1, aes(x = texture_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Texture Mean",
       x = "Texture Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h3 <- ggplot(breast_cancer1, aes(x = perimeter_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Perimeter Mean",
       x = "Perimeter Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h4 <- ggplot(breast_cancer1, aes(x = area_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Area Mean",
       x = "Area Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h5 <- ggplot(breast_cancer1, aes(x = smoothness_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Smoothness Mean",
       x = "Texture Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h6 <- ggplot(breast_cancer1, aes(x = compactness_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Compactness Mean",
       x = "Compactness Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h7 <- ggplot(breast_cancer1, aes(x = concavity_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Concavity Mean",
       x = "Concavity Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h8 <- ggplot(breast_cancer1, aes(x = concave.points_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Concave Points Mean",
       x = "Concave Points Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h9 <- ggplot(breast_cancer1, aes(x = symmetry_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Symmetry Mean",
       x = "Symmetry Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 

h10 <- ggplot(breast_cancer1, aes(x = fractal_dimension_mean, fill = as.factor(diagnosis))) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 30) +
  labs(title = "Fractal Dimension Mean",
       x = "Fractal Dimension Mean",
       y = "Frequency") +
  scale_fill_manual(values = c("0" = "#00AFBB", "1" = "#E7B800")) 


library(gridExtra)
grid.arrange(h1, h2, h3, h4, h5, h6, h7, h8, h9, h10, ncol = 2)
```

```{r}
# Scatterplots by patient ID

library(ggplot2)
# radius 
plot1<- ggplot(breast_cancer, aes(radius_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))

# texture
plot2 <- ggplot(breast_cancer, aes(texture_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))

# perimeter
plot3 <- ggplot(breast_cancer, aes(perimeter_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))

# area
plot4 <- ggplot(breast_cancer, aes(area_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))

# smoothness 
plot5 <- ggplot(breast_cancer, aes(smoothness_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))


# compactness 
plot6 <- ggplot(breast_cancer, aes(compactness_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))


# concavity
plot7 <- ggplot(breast_cancer, aes(concavity_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))


# concave points 
plot8 <- ggplot(breast_cancer, aes(concave.points_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))

# symmetry 
plot9 <- ggplot(breast_cancer, aes(symmetry_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))


# fractal dimension
plot10 <- ggplot(breast_cancer, aes(fractal_dimension_mean, id)) +
  geom_point(aes(color=diagnosis)) +
  scale_color_manual(values=c('#00AFBB', '#E7B800'))


library(gridExtra)
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9, plot10, ncol = 2)

```

```{r}

pairs(breast_cancer1[, c("radius_mean", "texture_mean", "perimeter_mean", "area_mean", "smoothness_mean", "compactness_mean", "concavity_mean", "concave.points_mean", "symmetry_mean", "fractal_dimension_mean")],
      col = c("#00AFBB", "#E7B800")[breast_cancer1$diagnosis + 1], lower.panel = NULL)
par(xpd = TRUE)
legend("bottomleft", legend = c("B", "M"), col = c("#00AFBB", "#E7B800"), pch = 1)
```

```{r}
# Boxplots
library(ggplot2)
library(gridExtra)

# Create a list to store the plots
boxplots_list <- list()

for (i in 2:10) {
  feature_name <- colnames(breast_cancer)[i + 1]  
  
  # Create a boxplot for the current feature
  boxplots_list[[i]] <- ggplot(breast_cancer, aes(x = diagnosis, y = .data[[feature_name]], fill = diagnosis)) +
    geom_boxplot() +
    scale_fill_manual(values = c('#00AFBB', '#E7B800')) +
    labs(title = paste(feature_name),
         x = "Diagnosis",
         y = feature_name)
}

grid.arrange(grobs = boxplots_list, ncol = 2)


```

```{r}
# correlation matrix 
library(corrplot)

cor_matrix <- cor(breast_cancer1)
corrplot(
  cor_matrix,
  tl.cex = 1,  
  tl.col = "black", 
  method = 'color'
)


```

### Part 3: Random Forest

```{r}
# split into training and test sets 
set.seed(123)
sample_indices <- sample(1:nrow(breast_cancer1), 0.7 * nrow(breast_cancer1))

train_set <- breast_cancer1[sample_indices, ]

test_set <- breast_cancer1[-sample_indices, ]

head(train_set)
head(test_set)
```

```{r}
library(randomForest)
set.seed(123)
# train random forest model
rf <- randomForest(as.factor(diagnosis) ~ ., data = train_set, ntree = 500)
print(rf)

# training set confusion matrix
rf.preds <- predict(rf, data = train_set)
confusion_matrix0 <- table(rf.preds, train_set$diagnosis)
confusion_matrix0

# make predictions on test set 
rf.predictions <- predict(rf, newdata = test_set)

# test set confusion matrix 
confusion_matrix1 <- table(rf.predictions, test_set$diagnosis)
confusion_matrix1

# variable importance 
varImpPlot(rf, main = "Random Forest Variable Importance")

# Calculate classification metrics (Train)
accuracy1 <- sum(diag(confusion_matrix0)) / sum(confusion_matrix0)
precision1 <- confusion_matrix0[2, 2] / sum(confusion_matrix0[, 2])
recall1 <- confusion_matrix0[2, 2] / sum(confusion_matrix0[2, ])
f1_score1 <- 2 * (precision1 * recall1) / (precision1 + recall1)

print(paste("Accuracy:", accuracy1))
print(paste("Precision:", precision1))
print(paste("Recall:", recall1))
print(paste("F1-Score:", f1_score1))

# Calculate classification metrics (Test)
accuracy <- sum(diag(confusion_matrix1)) / sum(confusion_matrix1)
precision <- confusion_matrix1[2, 2] / sum(confusion_matrix1[, 2])
recall <- confusion_matrix1[2, 2] / sum(confusion_matrix1[2, ])
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-Score:", f1_score))

#train error 
train.rf.error <- mean(rf.preds != train_set$diagnosis)

# test error 
test.rf.error2 <- mean(rf.predictions != test_set$diagnosis)

print(paste("Training Error:", train.rf.error))
print(paste("Test Error:", test.rf.error2))

```

### Part 4: SVM

```{r}
# find best cost for linear kernal 

library(e1071)
set.seed(123)
tune1 <- tune(e1071::svm, diagnosis ~ ., data = train_set, kernel = "linear", ranges = list(cost = c(seq(0.01,0.1,by = 0.1), seq(0.1,1,by = 0.1), seq(1,10, by = 1), seq(10,50, by = 10))))

summary(tune1)

```

```{r}
# pull out best cost 
c <- tune1$performances
c[c$error == min(c$error),]

best.cost <- tune1$best.model$cost
```

```{r}
# compute training and test errors with best cost 
library(e1071)
set.seed(123)
svm_bcost <- svm(diagnosis ~ ., data = train_set, kernel = "linear", type = "C-classification", cost = best.cost)

pred_train <- predict(svm_bcost, train_set)
train.error <- mean(pred_train != train_set$diagnosis)

confm <- table(Predicted = pred_train, Actual = train_set$diagnosis)
confm

pred_test <- predict(svm_bcost, test_set)
test.error <- mean(pred_test != test_set$diagnosis)

confm1 <- table(Predicted = pred_test, Actual = test_set$diagnosis)
confm1

linear_svm <- data.frame("Kernal" = "Linear", "Cost" = best.cost, "Training Error" = train.error, "Testing Error" = test.error)

linear_svm

```

```{r}
# radial kernal 

library(e1071)
set.seed(123)
tune2 <- tune(e1071::svm, diagnosis ~ ., data = train_set, kernel = "radial", ranges = list(cost = c(seq(0.01,0.1,by = 0.1), seq(0.1,1,by = 0.1), seq(1,10, by = 1)),
                                                                                            gamma = c(seq(0.1,1,by=0.1), seq(1,10,by=1))))

summary(tune2)

# extract best cost 

c2 <- tune2$performances
c2[c2$error == min(c2$error),]

best.cost1 <- tune2$best.model$cost
best.gamma1 <- tune2$best.model$gamma

# training and test errors 

svm_bcost1 <- svm(diagnosis ~ ., data = train_set, kernel = "radial", type = "C-classification", cost = best.cost1, gamma = best.gamma1)

pred_train1 <- predict(svm_bcost1, train_set)
train.error1 <- mean(pred_train1 != train_set$diagnosis)

confm2 <- table(Predicted = pred_train1, Actual = train_set$diagnosis)
confm2

pred_test1 <- predict(svm_bcost1, test_set)
test.error1 <- mean(pred_test1 != test_set$diagnosis)

confm3 <- table(Predicted = pred_test1, Actual = test_set$diagnosis)
confm3

radial_svm <- data.frame("Kernal" = "Radial", "Cost" = best.cost1, "Training Error" = train.error1, "Testing Error" = test.error1)

radial_svm

```

```{r}
# polynomial kernal 

library(e1071)
set.seed(123)
tune3 <- tune(e1071::svm, diagnosis ~ ., data = train_set, kernel = "polynomial", ranges = list(cost = c(seq(0.01,0.1,by = 0.1), seq(0.1,1,by = 0.1), seq(1,10, by = 1), seq(10,50, by = 10))))

summary(tune3)

# extract best cost 

c3 <- tune3$performances
c3[c3$error == min(c3$error),]

best.cost2 <- tune3$best.model$cost

# training and test errors 

svm_bcost2 <- svm(diagnosis ~ ., data = train_set, kernel = "polynomial", type = "C-classification", cost = best.cost2)

pred_train2 <- predict(svm_bcost2, train_set)
train.error2 <- mean(pred_train2 != train_set$diagnosis)

confm4 <- table(Predicted = pred_train1, Actual = train_set$diagnosis)
confm4

pred_test2 <- predict(svm_bcost2, test_set)
test.error2 <- mean(pred_test2 != test_set$diagnosis)

confm5 <- table(Predicted = pred_test2, Actual = test_set$diagnosis)
confm5

polynomial_svm <- data.frame("Kernal" = "Polynomial", "Cost" = best.cost2, "Training Error" = train.error2, "Testing Error" = test.error2)

polynomial_svm
```

```{r}
final <- rbind(linear_svm, radial_svm, polynomial_svm)
final

# radial performs the best
```

```{r}
# all other metrics for radial kernal svm model 
svm_bcost1 <- svm(diagnosis ~ ., data = train_set, kernel = "radial", gamma = best.gamma1, type = "C-classification", cost = best.cost1)
summary(svm_bcost1)
```

```{r}
# confusion matrices 
confm2
confm3
```

```{r}
# accuracy 
acc_train <- sum(diag(confm2)) / sum(confm2)
print(paste("Accuracy (Train):", acc_train))

acc_test <- sum(diag(confm3)) / sum(confm3)
print(paste("Accuracy (Test):", acc_test))

# precision
prec_train <- confm2[2,2] / sum(confm2[, 2])
print(paste("Precision (Train):", prec_train))

prec_test <- confm3[2,2] / sum(confm3[, 2])
print(paste("Precision (Test):", prec_test))

# recall
recall_train <- confm2[2,2] / sum(confm2[2, ])
print(paste("Recall (Train):", recall_train))

recall_test <- confm3[2,2] / sum(confm3[2, ])
print(paste("Recall (Test):", recall_test))

# f1
f1_train <- 2 * (prec_train * recall_train) / (prec_train + recall_train)
print(paste("F1 Score (Train):", f1_train))

f1_test <- 2 * (prec_test * recall_test) / (prec_test + recall_test)
print(paste("F1 Score (Test):", f1_test))
```

### Part 5: Compare the Models

```{r}
# ROC curve

library(pROC)
preds <- predict(svm_bcost1, newdata = test_set)

roc_curve1 <- roc(test_set$diagnosis, as.numeric(preds))
roc_curve2 <- roc(test_set$diagnosis, as.numeric(rf.predictions))
auc1 <- auc(roc_curve1)
auc2 <- auc(roc_curve2)

auc1 
auc2


plot(roc_curve1, main = "ROC Curves", col = "blue", lwd = 2)
plot(roc_curve2, col = "red", lwd = 2, add = TRUE)
legend("bottomright", legend = c("SVM", "Random Forest"), col = c("blue", "red"), lwd = 2)

```

```{r}
# comparing the 2 models 
# Assuming you have the same metrics for both methods

rf.metrics <- data.frame("Method" = "Random Forest", 
                         "Training Error" = train.rf.error, 
                         "Testing Error" = test.rf.error2,
                         "Accuracy (Training)" = accuracy1, 
                         "Precision (Training)" = precision1,
                         "Recall (Training)" = recall1, 
                         "F1 Score (Training)" = f1_score1, 
                         "Accuracy (Test)" = accuracy, 
                         "Precision (Test)" = precision,
                         "Recall (Test)" = recall, 
                         "F1 Score (Test)" = f1_score)

svm.metrics <- data.frame("Method" = "SVM", 
                          "Training Error" = train.error1, 
                          "Testing Error" = test.error1,
                          "Accuracy (Training)" = acc_train, 
                          "Precision (Training)" = prec_train,
                          "Recall (Training)" = recall_train, 
                          "F1 Score (Training)" = f1_train, 
                          "Accuracy (Test)" = acc_test, 
                          "Precision (Test)" = prec_test,
                          "Recall (Test)" = recall_test,
                          "F1 Score (Test)" = f1_test)

final1 <- rbind(rf.metrics, svm.metrics)
final1

library(knitr)
kable(final1)
```

### Discussion

Both the Random Forest and SVM models performed well on the data with relatively low error rates on both the training and test sets, and relatively high accuracies, recall, precision, and F1-scores. However, the SVM model seemed to perform slightly better than the Random Forest. The ROC curves quickly show that the SVM model performed slightly better. Ideally, the ROC curve should be as close to the top left corner as possible. Both curves have this ideal shape, but the SVM curve is higher. The area under the curve for the SVM model was 0.9521 as compared to 0.9111 in the random forest model. The closer the area under the curve is to 1, the more predictive power the model has, reinforcing that the SVM model was superior. In both models, the testing errors were only slightly higher than the training errors, suggesting good generalization. This is further demonstrated in the fact that the accuracy, precision, recall, and F1-scores were only marginally lower in the test sets than the training sets in both models. The training and testing error rates represent the level of misclassifications in the models, while accuracy represents the proportion of accurate diagnoses (both malignant and benign). Precision is the proportion of correctly predicting malignant cases, recall is the proportion of correctly identified malignancies out of all actual malignant cases, and the F1-score is a balanced measure that considers both precision and recall, providing an overall assessment of the model's ability to correctly classify both malignant and benign tumors. According to these metrics, the SVM outperformed the Random Forest model on both the training and the test sets with lower error rates and higher accuracy, precision, recall, and F1-scores. The accuracy of the SVM went from 96.75% on the training set to 95.91% on the test set, while the Random Forest changed from 94.47% accuracy on the training set to 91.81% on the test set. This pattern of a smaller difference between training and test set performance metrics in the SVM compared to the Random Forest is consistent for all metrics computed. This shows that the SVM might be better suited to handle new, unseen data than the Random Forest model. While the performance metrics for both the Random Forest and SVM models are quite impressive, the diagnostic nature of the data requires a level of predictive accuracy as close to perfect as possible. It is also necessary to discuss the limitations of this study. First, the dataset's size may influence the robustness of the models, and further validation on larger datasets is warranted to enhance generalizability. Additionally, the distribution of malignant and benign diagnoses was not evenly balanced, with a higher number of benign cases compared to malignant ones. This imbalance could potentially impact the model's ability to generalize effectively, as the algorithms may be more inclined to predict the majority class. Furthermore, the chosen features for analysis, although carefully selected, might not fully capture the intricate biological processes that underlie tumor behavior. It is important to recognize these limitations as they highlight the need for continued research and validation in diverse datasets to reinforce the reliability and applicability of the predictive models.

### Conclusion

In summary, the Random Forest and SVM models exhibited robust performance in predicting breast tumor malignancy, showcasing low error rates and high accuracy, precision, recall, and F1-scores. The generalization of both models was evident, and both models were proficient in correctly classifying tumors, but the SVM outperformed the Random Forest model across all metrics, emphasizing its suitability for handling new, unseen data. However, despite the impressive predictive ability of both models, just one misclassification could have catastrophic consequences. Achieving a near-perfect level of accuracy is particularly crucial in medical applications, where misclassifications could alter lives. The Random Forest and SVM models showcase promising predictive capabilities, but the pursuit of perfection in diagnostic accuracy remains a vital goal.
